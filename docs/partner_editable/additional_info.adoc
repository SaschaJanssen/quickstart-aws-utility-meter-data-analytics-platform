// Add steps as necessary for accessing the software, post-configuration, and testing. Donâ€™t include full usage instructions for your software, but add links to your product documentation for that information.
//Should any sections not be applicable, remove them

== Post deployment steps

To see the capabilities of the QuickStart, follow the below steps to use dataset of meter reads from the City of London, between 2011 to 2014.

=== Download Sample Dataset
Download the sample dataset from https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households[here^]. The file can be downloaded to your local machine unzipped and then uploaded to S3. However, given the size of the unzipped file (~11GB), it is faster to run a EC2 instance with sufficient privileges to write to S3, download the file to the EC2 instance, unzip it and upload the data from there.

https://data.london.gov.uk/download/smartmeter-energy-use-data-in-london-households/3527bf39-d93e-4071-8451-df2ade1ea4f2/Power-Networks-LCL-June2015(withAcornGps).zip[Download^]

=== Upload the dataset to the Raw (Landing zone) S3 bucket
The sample dataset needs to be uploaded to the landing zone bucket. The landing zone is the starting point for AWS Glue workflow. Files that are placed in this S3 bucket will be processed by the ETL pipeline. Furthermore, the AWS Glue ETL workflow tracks which files haven been processed and which are not.

To upload the sample dataset to the Raw S3 bucket, follow the below steps:

. Select the landing zone bucket, buckets names are generated, find the one which contains the word landing zone. This will be the starting point for the ETL process.
+
image::../images/1_bucket_layout.png[Bucket Layout]

. Upload the London Meter Data to the landing zone bucket.
+
image::../images/2_upload_demo_data_set.png[Upload demo data set]

. After the demo data is uploaded, the folder contains one file with all the meter reads.
+
image::../images/3_upload_demo_data_set.png[Uploaded demo data set]

//[TODO EC2 command line steps]

=== Prepare weather data if you want to use it for training and forecasting

. Download the sample weather dataset from https://www.kaggle.com/jeanmidev/smart-meters-in-london?select=weather_hourly_darksky.csv
. Upload the dataset to S3 bucket
. Create "weather" table in target Glue database using below SQL statement. Make sure replace "weather-data-location" to where you upload the weather dataset.
+
image::../images/4_create_weather_table.png[Create weather table via Athena]

```sql
CREATE EXTERNAL TABLE weather(
  visibility double,
  windbearing bigint,
  temperature double,
  time string,
  dewpoint double,
  pressure double,
  apparenttemperature double,
  windspeed double,
  preciptype string,
  icon string,
  humidity double,
  summary string )
ROW FORMAT DELIMITED
      FIELDS TERMINATED BY ','
LOCATION
  's3://<weather-data-location>'
TBLPROPERTIES (
  'has_encrypted_data'='false',
  'skip.header.line.count'='1');
```

If you have other weather dataset you want to use, just make sure it contains at least below 4 columns.

[cols="1,1,1,1", options="header"]
.Schema
|===
|Field
|Type
|Mandatory
|Format
|time |string|X|  yyyy-MM-dd HH:mm:ss
|temperature| double|X|
|apparenttemperature |double|X|
|humidity |double|X|
|===

In CloudFormation stack, set "WithWeather" to 1 to indicate weather data is available to use.

=== Start the AWS Glue ETL and ML training pipeline

By default, the pipeline will be triggered each day at 09:00 AM and process the newly arrived data in the landing zone S3 bucket. However, it's also possible to trigger the pipeline manually. To trigger the ETL pipeline, go to the AWS Glue console and then select the 'Workflow' menu entry. The AWS Glue workflow orchestrates the different steps of the ETL process. After the workflow run has successfully completed, a state machine https://state.machine[link] is called to build the ML model and prepare the data for energy usage forecast and anomaly.

Follow the below steps to start the ETL pipeline.

. Go to the AWS Glue Console, and select 'Workflows'
+
image::../images/4_start_etl_workflow.png[AWS Glue Console]

. Select the workflow and choose 'Run' from the drop-down-menu
+
image::../images/5_start_etl_workflow.png[Start ETL workflow]

. After started, the Workflow should be indicated as 'Running' in the 'History' tab
+
image::../images/6_start_etl_workflow.png[Start ETL workflow]

TIP: If the workflow doesn't start and jumps directly to 'Completed' go back to step 2

== Customize this Quick Start

This Quick Start can be easily customized to use with your own data format. To do that, the first AWS Glue job has to be adjusted to map the incoming data to the internal meter data lake schema. The following steps describe how it can be achieved.

The first AWS Glue Job in the AWS Glue ETL workflow transforms the raw data in the landing zone S3 bucket to cleaned data in the Clean Zone S3 bucket. This step is also responsible for mapping the inbound data to the internal data schema, which is used by the rest of the steps in the AWS ETL workflow and the ML step functions.

To update the data mapping, the AWS Glue job can be edited directly in the web editor.

. Navigate to the AWS Glue Job console
+
image::../images/1_edit_etl_job.png[AWS Glue Job console]

. Select the 'transform_raw_to_clean-*' job
+
image::../images/2_edit_etl_job.png[Edit ETL Job]

. Open script editor and start editing the input mapping
+
image::../images/3_open_editor.png[Open editor]

. To adopt a different input format look for the 'ApplyMapping' call and adjust it to your needs  to reflect your custom input format. Currently, the internal model works with the following schema:
+
[cols="1,1,1,1", options="header"]
.Schema
|===
|Field
|Type
|Mandatory
|Format

|meter_id| String| X|
|reading_value| double| X|0.000
|reading_type| String| X|AGG\|INT
|reading_date_time| Timestamp| X|yyyy-MM-dd HH:mm:ss.SSS
|date_str| String|X| yyyyMMdd
|obis_code| String| |
|week_of_year| int| |
|month| int| |
|year| int| |
|hour| int| |
|minute| int| |
|===

include::./ml_pipeline.adoc[]

== Data schema and API i/o format
include::./data_format.adoc[]